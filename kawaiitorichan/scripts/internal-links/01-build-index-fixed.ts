#!/usr/bin/env tsx
/**
 * Build an index of posts with PROPER Japanese phrase extraction
 */

import { getPayload } from 'payload'
import configPromise from '@payload-config'
import * as fs from 'fs/promises'
import * as path from 'path'
import * as crypto from 'crypto'

const OUTPUT_DIR = path.join(process.cwd(), 'data', 'internal-links')

interface PostIndex {
  id: string
  title: string
  slug: string
  excerpt?: string
  keywords: string[]
  anchorPhrases: string[]
  contentHash: string
  language: 'ja' | 'en'
  textContent: string
}

/**
 * Properly extract text from Lexical editor format
 * This handles nested structures and preserves complete text
 */
function extractTextFromLexical(node: any, result: string[] = []): string {
  if (!node) return result.join('')
  
  // Handle text nodes
  if (node.type === 'text' && node.text) {
    result.push(node.text)
    return result.join('')
  }
  
  // Handle nodes with children
  if (node.children && Array.isArray(node.children)) {
    for (const child of node.children) {
      extractTextFromLexical(child, result)
    }
  }
  
  return result.join('')
}

/**
 * Extract meaningful Japanese phrases that make good anchor text
 */
function extractJapanesePhrases(text: string): string[] {
  const phrases = new Set<string>()
  
  // Clean the text
  const cleanText = text
    .replace(/<[^>]*>/g, ' ')
    .replace(/[\n\r]+/g, ' ')
    .replace(/\s+/g, ' ')
    .trim()
  
  // Golf-specific compound terms (these are meaningful multi-word phrases)
  const golfCompounds = [
    // Technical swing terms
    '„Ç¥„É´„Éï„Çπ„Ç§„É≥„Ç∞', '„Çπ„Ç§„É≥„Ç∞„Éó„É¨„Éº„É≥', '„Éê„ÉÉ„ÇØ„Çπ„Ç§„É≥„Ç∞', '„ÉÄ„Ç¶„É≥„Çπ„Ç§„É≥„Ç∞',
    '„Éï„Ç©„É≠„Éº„Çπ„É´„Éº', '„Ç§„É≥„Éë„ÇØ„Éà„Çæ„Éº„É≥', '„Çπ„Ç§„É≥„Ç∞„Ç¢„Éº„ÇØ', '„ÉÜ„Éº„ÇØ„Éê„ÉÉ„ÇØ',
    '„Ç¢„Éâ„É¨„Çπ„Éù„Ç∏„Ç∑„Éß„É≥', '„Éà„ÉÉ„Éó„Ç™„Éñ„Çπ„Ç§„É≥„Ç∞', '„Çπ„Ç§„É≥„Ç∞„Éë„Çπ', '„Çπ„Ç§„É≥„Ç∞„ÉÜ„É≥„Éù',
    
    // Club and shot types
    '„Éâ„É©„Ç§„Éê„Éº„Ç∑„Éß„ÉÉ„Éà', '„Ç¢„Ç§„Ç¢„É≥„Ç∑„Éß„ÉÉ„Éà', '„Éë„Çø„Éº„Çπ„Éà„É≠„Éº„ÇØ', '„Ç¶„Çß„ÉÉ„Ç∏„Ç∑„Éß„ÉÉ„Éà',
    '„Éï„Çß„Ç¢„Ç¶„Çß„Ç§„Ç¶„ÉÉ„Éâ', '„É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£„ÇØ„É©„Éñ', '„Çµ„É≥„Éâ„Ç¶„Çß„ÉÉ„Ç∏', '„Éî„ÉÉ„ÉÅ„É≥„Ç∞„Ç¶„Çß„ÉÉ„Ç∏',
    '„É≠„Éñ„Ç¶„Çß„ÉÉ„Ç∏', '„ÇÆ„É£„ÉÉ„Éó„Ç¶„Çß„ÉÉ„Ç∏',
    
    // Course management
    '„ÉÜ„Ç£„Éº„Ç∑„Éß„ÉÉ„Éà', '„Ç¢„Éó„É≠„Éº„ÉÅ„Ç∑„Éß„ÉÉ„Éà', '„Éê„É≥„Ç´„Éº„Ç∑„Éß„ÉÉ„Éà', '„Ç∞„É™„Éº„É≥Âë®„Çä',
    '„Éî„É≥„Éù„Ç∏„Ç∑„Éß„É≥', '„Ç≥„Éº„Çπ„Éû„Éç„Ç∏„É°„É≥„Éà', '„ÇØ„É©„ÉñÈÅ∏Êäû', '„Ç∑„Éß„ÉÉ„ÉàÈÅ∏Êäû',
    '„Ç≥„Éº„ÇπÊà¶Áï•', '„Éõ„Éº„É´„É¨„Ç§„Ç¢„Ç¶„Éà', '„Ç∞„É™„Éº„É≥„É™„Éº„Éá„Ç£„É≥„Ç∞',
    
    // Practice and improvement
    '„Ç¥„É´„ÉïÁ∑¥Áøí', '„Ç¥„É´„Éï„É¨„ÉÉ„Çπ„É≥', 'Á∑¥ÁøíÊñπÊ≥ï', '‰∏äÈÅîÊñπÊ≥ï',
    'È£õË∑ùÈõ¢„Ç¢„ÉÉ„Éó', 'ÊñπÂêëÊÄßÂêë‰∏ä', '„Çπ„Ç≥„Ç¢„É°„Ç§„ÇØ', '„Éü„Çπ„Ç∑„Éß„ÉÉ„Éà',
    '„Çπ„Ç§„É≥„Ç∞ÊîπÂñÑ', '„Ç∞„É™„ÉÉ„ÉóÁ∑¥Áøí', '„Éë„ÉÉ„ÉÜ„Ç£„É≥„Ç∞Á∑¥Áøí', '„Ç∑„Éß„Éº„Éà„Ç≤„Éº„É†',
    
    // Equipment
    '„Ç¥„É´„Éï„ÇØ„É©„Éñ', '„Ç¥„É´„Éï„Éú„Éº„É´', '„Ç¥„É´„Éï„Ç∑„É•„Éº„Ç∫', '„Ç¥„É´„Éï„Ç∞„É≠„Éº„Éñ',
    '„Ç≠„É£„Éá„Ç£„Éê„ÉÉ„Ç∞', '„Ç¥„É´„Éï„Ç¶„Çß„Ç¢', '„É¨„É≥„Ç∏„Éï„Ç°„Ç§„É≥„ÉÄ„Éº', '„Çπ„Ç≥„Ç¢„Ç´„Éº„Éâ',
    
    // Rules and etiquette
    '„Ç¥„É´„Éï„É´„Éº„É´', '„Ç¥„É´„Éï„Éû„Éä„Éº', '„Ç®„ÉÅ„Ç±„ÉÉ„Éà', '„É≠„Éº„Ç´„É´„É´„Éº„É´',
    '„Éö„Éä„É´„ÉÜ„Ç£„Çπ„Éà„É≠„Éº„ÇØ', '„Ç¢„É≥„Éó„É¨„Ç§„Ç¢„Éñ„É´', '„Ç¶„Ç©„Éº„Çø„Éº„Éè„Ç∂„Éº„Éâ', 'OB„É©„Ç§„É≥',
    
    // Mental game
    '„É°„É≥„Çø„É´„Ç≤„Éº„É†', '„Éó„É¨„ÉÉ„Ç∑„É£„ÉºÂØæÂá¶', 'ÈõÜ‰∏≠ÂäõÂêë‰∏ä', '„É´„Éº„ÉÜ„Ç£„É≥Á¢∫Á´ã',
    '„Éó„É¨„Ç∑„Éß„ÉÉ„Éà„É´„Éº„ÉÜ„Ç£„É≥', '„Éù„Çπ„Éà„Ç∑„Éß„ÉÉ„Éà„É´„Éº„ÉÜ„Ç£„É≥', '„Éì„Ç∏„É•„Ç¢„É©„Ç§„Çº„Éº„Ç∑„Éß„É≥',
    
    // Common terms for beginners
    'ÂàùÂøÉËÄÖ„Ç¥„É´„Éï„Ç°„Éº', 'ÂàùÂøÉËÄÖÂêë„Åë', '„Ç¥„É´„ÉïÂÖ•ÈñÄ', '„Ç¥„É´„Éï„Éá„Éì„É•„Éº',
    'Âü∫Êú¨„Çπ„Ç§„É≥„Ç∞', 'Âü∫Êú¨„Ç∞„É™„ÉÉ„Éó', 'Âü∫Êú¨„Çπ„Çø„É≥„Çπ', 'Âü∫Êú¨„Ç¢„Éâ„É¨„Çπ'
  ]
  
  // First, extract all predefined golf compound terms
  for (const compound of golfCompounds) {
    if (cleanText.includes(compound)) {
      phrases.add(compound)
    }
  }
  
  // Extract natural Japanese phrases from the text
  // Split by punctuation to get sentence fragments
  const sentences = cleanText.split(/[„ÄÇ„ÄÅÔºÅÔºü„Äå„Äç„Äé„ÄèÔºàÔºâ]/g)
  
  for (const sentence of sentences) {
    const trimmed = sentence.trim()
    if (!trimmed) continue
    
    // Look for noun phrases connected by particles
    // Pattern: [ÂêçË©û/ÂãïË©û]„ÅÆ[ÂêçË©û] (e.g., "„Çπ„Ç§„É≥„Ç∞„ÅÆÂü∫Êú¨")
    const nounPhrases = trimmed.match(/[„Ç°-„É∂„Éº‰∏Ä-Èæ†]{2,}[„ÅÆ„Å®„Åß][„Ç°-„É∂„Éº‰∏Ä-Èæ†]{2,}/g)
    if (nounPhrases) {
      for (const phrase of nounPhrases) {
        if (phrase.length >= 4 && phrase.length <= 15) {
          phrases.add(phrase)
        }
      }
    }
    
    // Look for compound katakana + kanji terms (common in golf)
    const compoundTerms = trimmed.match(/[„Ç°-„É∂„Éº]{3,}[‰∏Ä-Èæ†]{1,4}/g)
    if (compoundTerms) {
      for (const term of compoundTerms) {
        if (term.length >= 4 && term.length <= 12) {
          phrases.add(term)
        }
      }
    }
    
    // Look for verb phrases (e.g., "È£õË∑ùÈõ¢„Çí‰º∏„Å∞„Åô")
    const verbPhrases = trimmed.match(/[‰∏Ä-Èæ†]{2,}[„Çí„Åå„Å´][‰∏Ä-Èæ†]{2,}[„Åô„Çã„Åè„ÅÑ„ÅÜ„Å§„ÇÄ„Å∂„Å¨„Åê„Åö„Åò]/g)
    if (verbPhrases) {
      for (const phrase of verbPhrases) {
        if (phrase.length >= 5 && phrase.length <= 15) {
          phrases.add(phrase)
        }
      }
    }
    
    // Look for descriptive phrases (ÂΩ¢ÂÆπË©û + ÂêçË©û)
    const descriptivePhrases = trimmed.match(/[ËâØÊÇ™Ê≠£Á¢∫ÈÅ©ÂàáÂü∫Êú¨ÈáçË¶ÅÂ§ßÂ∞èÈ´ò‰ΩéÊó©ÈÅÖÂº∑Âº±][„ÅÑ„Åó„Åè„Åã„Å™]*[„Ç°-„É∂„Éº‰∏Ä-Èæ†]{2,}/g)
    if (descriptivePhrases) {
      for (const phrase of descriptivePhrases) {
        if (phrase.length >= 3 && phrase.length <= 10) {
          phrases.add(phrase)
        }
      }
    }
  }
  
  // Extract important standalone golf terms (but only if they're meaningful)
  const importantTerms = [
    '„Éâ„É©„Ç§„Éê„Éº', '„Ç¢„Ç§„Ç¢„É≥', '„Éë„Çø„Éº', '„Ç¶„Çß„ÉÉ„Ç∏', '„Éï„Çß„Ç¢„Ç¶„Çß„Ç§',
    '„Ç∞„É™„Éº„É≥', '„Éê„É≥„Ç´„Éº', '„É©„Éï', '„ÉÜ„Ç£„Éº', '„Éî„É≥', '„Ç´„ÉÉ„Éó',
    '„Çπ„Ç§„É≥„Ç∞', '„Ç∞„É™„ÉÉ„Éó', '„Çπ„Çø„É≥„Çπ', '„Ç¢„Éâ„É¨„Çπ', '„Ç§„É≥„Éë„ÇØ„Éà',
    '„Éï„Ç©„É≠„Éº', '„Éï„Ç£„Éã„ÉÉ„Ç∑„É•', '„ÉÜ„É≥„Éù', '„É™„Ç∫„É†', '„Çø„Ç§„Éü„É≥„Ç∞',
    '„Çπ„É©„Ç§„Çπ', '„Éï„ÉÉ„ÇØ', '„Éâ„É≠„Éº', '„Éï„Çß„Éº„Éâ', '„Éó„ÉÉ„Ç∑„É•', '„Éó„É´',
    '„Éà„ÉÉ„Éó', '„ÉÄ„Éï„Çä', '„Ç∑„É£„É≥„ÇØ', '„ÉÅ„Éº„Éî„É≥', '„ÉÜ„É≥„Éó„É©',
    '„Éë„Éº', '„Éê„Éº„Éá„Ç£', '„Ç§„Éº„Ç∞„É´', '„Éú„ÇÆ„Éº', '„Ç¢„É´„Éê„Éà„É≠„Çπ',
    '„Éè„É≥„Éá„Ç£„Ç≠„É£„ÉÉ„Éó', '„Çπ„Ç≥„Ç¢', '„Çπ„Éà„É≠„Éº„ÇØ', '„É§„Éº„Éâ', '„É°„Éº„Éà„É´'
  ]
  
  for (const term of importantTerms) {
    if (cleanText.includes(term)) {
      // Only add standalone terms if they appear in a meaningful context
      const contextPattern = new RegExp(`[„Ç°-„É∂„Éº‰∏Ä-Èæ†]*${term}[„Ç°-„É∂„Éº‰∏Ä-Èæ†]*`, 'g')
      const matches = cleanText.match(contextPattern)
      if (matches) {
        for (const match of matches) {
          // Add the extended phrase if it's longer than just the term
          if (match.length > term.length) {
            phrases.add(match)
          } else {
            // Add the term itself if it appears standalone
            phrases.add(term)
          }
        }
      }
    }
  }
  
  // Filter and sort phrases
  const finalPhrases = Array.from(phrases)
    .filter(phrase => {
      // Remove single characters
      if (phrase.length < 2) return false
      // Remove phrases that are just particles
      if (/^[„ÅÆ„Åå„Çí„ÅØ„Åß„Å´„Å∏„Å®„ÇÇ]+$/.test(phrase)) return false
      // Keep everything else
      return true
    })
    // Sort by relevance: longer phrases first, then by frequency in text
    .sort((a, b) => {
      // Prioritize golf-specific compounds
      const aIsCompound = golfCompounds.includes(a)
      const bIsCompound = golfCompounds.includes(b)
      if (aIsCompound && !bIsCompound) return -1
      if (!aIsCompound && bIsCompound) return 1
      
      // Then by length (longer = more specific)
      if (b.length !== a.length) return b.length - a.length
      
      // Then alphabetically
      return a.localeCompare(b)
    })
    .slice(0, 100) // Limit to top 100 phrases
  
  return finalPhrases
}

/**
 * Detect language of text
 */
function detectLanguage(text: string): 'ja' | 'en' {
  const japaneseChars = text.match(/[„ÅÅ-„Çì„Ç°-„É∂„Éº‰∏Ä-Èæ†]/g) || []
  const englishChars = text.match(/[a-zA-Z]/g) || []
  return japaneseChars.length > englishChars.length ? 'ja' : 'en'
}

/**
 * Generate content hash
 */
function generateContentHash(content: string): string {
  return crypto.createHash('md5').update(content).digest('hex')
}

async function buildIndex() {
  console.log('Building FIXED post index for internal linking...')
  
  const payload = await getPayload({ config: configPromise })
  
  try {
    // Fetch all published posts
    const { docs: posts } = await payload.find({
      collection: 'posts',
      limit: 1000,
      where: {
        _status: { equals: 'published' }
      },
      depth: 1 // Include related fields
    })
    
    console.log(`Found ${posts.length} published posts`)
    
    const indexedPosts: PostIndex[] = []
    
    for (const post of posts) {
      // Properly extract text from Lexical content
      const textContent = extractTextFromLexical(post.content?.root)
      const language = detectLanguage(textContent + ' ' + (post.title || ''))
      
      // Extract anchor phrases based on language
      let anchorPhrases: string[] = []
      if (language === 'ja') {
        anchorPhrases = extractJapanesePhrases(textContent + ' ' + (post.title || ''))
      }
      
      // Extract keywords from various sources
      const keywords: Set<string> = new Set()
      
      // Add title-based phrases
      if (post.title && language === 'ja') {
        const titlePhrases = extractJapanesePhrases(post.title)
        titlePhrases.forEach(phrase => keywords.add(phrase))
      }
      
      // Add SEO keywords if available
      if (post.meta?.keywords) {
        post.meta.keywords.split(',').forEach((kw: string) => {
          const cleaned = kw.trim()
          if (cleaned.length >= 2) keywords.add(cleaned)
        })
      }
      
      // Add category names
      if (post.categories && Array.isArray(post.categories)) {
        for (const cat of post.categories) {
          if (typeof cat === 'object' && cat.title) {
            keywords.add(cat.title)
          }
        }
      }
      
      const indexed: PostIndex = {
        id: String(post.id),
        title: post.title || '',
        slug: post.slug || '',
        excerpt: post.excerpt || '',
        keywords: Array.from(keywords),
        anchorPhrases: anchorPhrases,
        contentHash: generateContentHash(textContent),
        language,
        textContent: textContent.substring(0, 5000) // Store first 5000 chars for reference
      }
      
      indexedPosts.push(indexed)
      
      // Show sample phrases for debugging
      if (anchorPhrases.length > 0) {
        console.log(`‚úì ${post.title}`)
        console.log(`  Sample phrases: ${anchorPhrases.slice(0, 5).join(', ')}`)
      } else {
        console.log(`‚ö† ${post.title} (no phrases extracted)`)
      }
    }
    
    // Create output directory
    await fs.mkdir(OUTPUT_DIR, { recursive: true })
    
    // Save index
    const outputPath = path.join(OUTPUT_DIR, 'posts-index.json')
    await fs.writeFile(
      outputPath,
      JSON.stringify({ posts: indexedPosts, timestamp: new Date().toISOString() }, null, 2)
    )
    
    console.log('\n‚úÖ Index built successfully!')
    console.log(`üìÅ Saved to: ${outputPath}`)
    console.log(`üìä Total posts indexed: ${indexedPosts.length}`)
    
    // Statistics
    const japanesePosts = indexedPosts.filter(p => p.language === 'ja')
    const withPhrases = japanesePosts.filter(p => p.anchorPhrases.length > 0)
    const avgPhrases = japanesePosts.reduce((sum, p) => sum + p.anchorPhrases.length, 0) / Math.max(japanesePosts.length, 1)
    
    console.log('\nüìà Statistics:')
    console.log(`  - Japanese posts: ${japanesePosts.length}`)
    console.log(`  - Posts with phrases: ${withPhrases.length}`)
    console.log(`  - Avg phrases per Japanese post: ${Math.round(avgPhrases)}`)
    
  } catch (error) {
    console.error('‚ùå Error building index:', error)
    process.exit(1)
  }
  
  process.exit(0)
}

buildIndex().catch(console.error)